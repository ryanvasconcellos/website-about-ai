<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Inteligência Artificial</title>
    <link rel="stylesheet" href="estilos/index.css">
    <link rel="stylesheet" href="estilos/media.css">
    <link rel="shortcut icon" href="imagens/favicon.png" type="image/x-icon">
</head>
<body>
    <header>
        <h1>Inteligência Artificial</h1>
        <p>Machine Learning, Ciência de Dados e Seus Impactos no Futuro das Profissões, Sociedade e Economia.</p>
    </header>
    <nav>
            <a href="profissoes.html">Profissões</a></li>
            <a href="desafios.html">Desafios</a></li>
            <a href="pros.html">Prós e Contras</a></li>
    </nav>
    <main>
        <article>
            <h1>Resumo</h1>
            <p>Este artigo tem como objetivo explorar o desenvolvimento e a inter-relação entre <strong><a href="https://www.nasa.gov/what-is-artificial-intelligence/" target="_blank">Inteligência Artificial (IA)</a>, <a href="https://www.sas.com/pt_br/insights/analytics/machine-learning.html" target="_blank">Machine Learning (ML)</a>, <a href="https://aws.amazon.com/pt/what-is/data-science/" target="_blank">Ciência de Dados</a></strong> e suas implicações nas profissões, na economia e na sociedade. A pesquisa aborda a criação e a evolução da IA, as primeiras aplicações da tecnologia, os impactos nas profissões, a substituição de empregos, o uso da IA em áreas como saúde e educação, os desafios éticos associados, e os desenvolvimentos futuros até 2030. Além disso, são discutidos os avanços tecnológicos, a disseminação e controle de fake news, e o papel da IA na segurança e nos conflitos bélicos, fornecendo uma visão holística sobre os potenciais e riscos dessa tecnologia emergente.</p>
            <section class="imagaem" id="01">
                
            <div class="container">
                <img class="imagem" src="imagens/pag-index/woman-700-index.jpg" alt="Inteligência Artificial">
            </div>
                
            </section>
            <h1>Introdução</h1>
            <p>A <strong>Inteligência Artificial (IA)</strong>, uma das áreas mais inovadoras e impactantes da ciência da computação, vem sendo amplamente aplicada em diversas áreas do conhecimento humano. Desde suas origens nas décadas de 1950 e 1960 até sua implementação em uma vasta gama de indústrias, a IA tem se expandido, trazendo avanços significativos para a sociedade, mas também levantando questões sobre ética, segurança e desigualdade. A <strong>Ciência de Dados</strong> e o <strong>Machine Learning</strong> (ML) surgiram como componentes cruciais dessa evolução, permitindo que as máquinas "aprendam" e tomem decisões mais precisas. Este artigo analisa como a IA está transformando as profissões, quais empregos podem ser substituídos, e os efeitos de suas aplicações na sociedade e na economia.
            </p>
            <h1>O Surgimento da Inteligência Artificial</h1>
            <h2>A Criação da IA</h2>
            <p>O conceito de Inteligência Artificial foi formalizado no final da década de 1950 por <a href="http://jmc.stanford.edu/index.html" target="_blank">John McCarthy</a>, que cunhou o termo durante a <a href="https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth" target="_blank">conferência de Dartmouth</a> em 1956. A proposta inicial da IA era que máquinas poderiam simular processos de inteligência humana, como raciocínio lógico e aprendizado. <strong><a href="https://www.techtudo.com.br/guia/2023/04/quem-foi-alan-turing-saiba-a-historia-do-pai-da-ciencia-da-computacao-edinfoeletro.ghtml" target="_blank">Alan Turing</a></strong>, um dos principais pioneiros da computação, introduziu o conceito de <strong>máquinas pensantes</strong>, onde uma máquina seria considerada inteligente se pudesse enganar um ser humano em uma conversa, o que ficou conhecido como <strong><a href="https://www2.ifsc.usp.br/portal-ifsc/teste-de-turing-e-inteligencia-artificial/" target="_blank">Teste de Turing</a></strong>.</p>

            <div class="container">
                <img class="imagem" src="imagens/pag-index/turing-test-700-index.jpg" alt="Teste de Turing">
            </div>

            <p>O Teste de Turing, criado por Alan Turing em 1950, avalia se uma máquina consegue imitar um ser humano em uma conversa por texto a ponto de enganar um juiz humano. Se o juiz não souber dizer quem é a máquina, ela passa no teste. Embora não prove que a máquina pensa, o teste mede sua habilidade de simular inteligência humana.</p>
            <h2>Primeiras Fases e Limitações</h2>
            <p>Nos primeiros anos da inteligência artificial, especialmente entre as décadas de 1950 e 1980, os sistemas eram construídos com base na chamada <strong><a href="https://www.ultralytics.com/pt/glossary/symbolic-ai" target="_blank">IA simbólica</a></strong>. Nesse modelo, os pesquisadores criavam algoritmos que seguiam regras lógicas definidas manualmente. Os programas eram alimentados com vastas quantidades de conhecimento explícito — por exemplo, “se X acontece, então faça Y”. Isso funcionava relativamente bem em ambientes controlados e com problemas bem definidos, como jogos de tabuleiro ou diagnósticos simples.
            </p>
            <p>
                No entanto, conforme os desafios se tornavam mais complexos e exigiam capacidade de adaptação, aprendizado e interpretação de dados ambíguos ou incompletos, esse tipo de abordagem começou a mostrar suas limitações. A IA simbólica não conseguia lidar com a imprecisão do mundo real e exigia um esforço imenso para programar todas as possíveis variáveis. Além disso, o poder computacional da época era insuficiente para rodar sistemas mais sofisticados em larga escala. Embora esse tipo de IA fosse eficaz para resolver problemas simples, sua aplicação se limitava devido à falta de flexibilidade e à complexidade das tarefas mais avançadas. Esse período, conhecido como <strong><a href="https://www.guilhermefavaron.com.br/post/os-invernos-da-ia-ciclos-de-ascensao-e-queda-na-historia-da-inteligencia-artificial" target="_blank">"inverno da IA"</a></strong>, ocorreu principalmente entre as décadas de 1970 e 1980, quando os avanços eram lentos devido a restrições de hardware e de compreensão sobre o funcionamento da inteligência.

            </p>
            <p>
                Foi apenas a partir dos anos 1990 e 2000, com o crescimento do poder computacional, o surgimento de <strong>grandes volumes de dados <a href="https://www.sas.com/en_ca/insights/big-data/what-is-big-data.html" target="_blank">(big data)</strong></a> e avanços em algoritmos estatísticos e de aprendizado de máquina, que a IA voltou a ganhar força — desta vez, com abordagens subsimbolistas, como as <strong><a href="https://aws.amazon.com/pt/what-is/neural-network/" target="_blank">redes neurais</a></strong> e o <strong><a href="https://www.ibm.com/think/topics/deep-learning" target="_blank">aprendizado profundo (deep learning)</a></strong>, que hoje dominam o campo.
            </p>
            <h1>A Evolução da IA e a Ascensão do Machine Learning</h1>
            <h2>A Revolução do Machine Learning</h2>
            <p>O cenário mudou nos anos 90, quando surgiu o <strong>Machine Learning (ML)</strong>, um subcampo da IA que permite que as máquinas aprendam com os dados, sem serem explicitamente programadas para isso. O ML depende de algoritmos e modelos matemáticos que identificam padrões em grandes conjuntos de dados. Em contraste com a <strong>IA simbólica</strong>, onde tudo é programado de forma manual, o ML permite que os sistemas se adaptem e melhorem com o tempo.
            </p>
            <p>
                Esse avanço representou uma ruptura significativa em relação aos métodos anteriores, pois permitiu que os sistemas aprendessem diretamente a partir dos dados, sem a necessidade de programar manualmente todas as regras. Com isso, modelos passaram a superar o desempenho humano em algumas tarefas específicas, como a classificação de imagens e o reconhecimento de fala. Além disso, o deep learning abriu caminho para aplicações antes consideradas inviáveis, como assistentes virtuais, carros autônomos e sistemas de recomendação altamente personalizados. A combinação de grandes quantidades de dados, algoritmos otimizados e poder computacional — especialmente com o uso de <strong><a href="https://aws.amazon.com/pt/what-is/gpu/" target="_blank">GPUs</a></strong> — consolidou o deep learning como o motor da atual revolução da inteligência artificial, tema que exploraremos com mais profundidade a seguir.
            </p>
            <h2>Deep Learning e a Nova Era da IA</h2>
            <p>
                Na década de 2000, com o aumento da capacidade de processamento computacional e a disponibilidade de grandes volumes de dados, o Deep Learning (aprendizado profundo) emergiu como uma das abordagens mais eficazes para resolver problemas complexos. As <strong><a href="https://www.datacamp.com/pt/tutorial/introduction-to-deep-neural-networks" target="_blank">redes neurais profundas</a></strong> — inspiradas no cérebro humano — são capazes de aprender em múltiplas camadas de <strong><a href="https://sol.sbc.org.br/index.php/educomp_estendido/article/view/19398" target="_blank">abstração</a></strong>, o que tornou possível realizar tarefas como reconhecimento de imagem, processamento de linguagem natural e tradução automática com grande precisão.
            </p>
            <p>
                Com o tempo, o deep learning passou a ser aplicado em diversas áreas, da medicina à indústria, impulsionando inovações que vão desde diagnósticos por imagem até sistemas preditivos de manutenção. Um dos fatores determinantes para esse avanço foi a capacidade de treinar modelos com grandes conjuntos de dados, permitindo que os algoritmos detectassem padrões com um nível de sofisticação antes inalcançável. No entanto, a eficácia dessas redes depende diretamente da qualidade, diversidade e volume dos dados utilizados em seu treinamento — o que trouxe à tona a importância estratégica da ciência de dados nesse novo cenário.
            </p>
            <p>
                É nesse contexto que a ciência de dados se torna peça-chave, funcionando como a ponte entre o mundo dos <a href="https://revistas.pucsp.br/emp/article/view/53409" target="_blank">dados brutos</a> e os sistemas de IA capazes de gerar valor a partir deles. No tópico a seguir, exploraremos como a ciência de dados contribui para o desenvolvimento da inteligência artificial, desde a coleta e preparação dos dados até a modelagem, avaliação e interpretação dos resultados.
            </p>
            <h1>Ciência de Dados: O Papel na IA</h1>
            <h2>Defiinição de Ciência de Dados</h2>
            <p>
                A Ciência de Dados é a interseção entre matemática, estatística, computação e <a href="https://codingdatatoday.co/blog/glossario/conhecimento-de-dominio/" target="_blank">conhecimento de domínio</a>, que visa extrair insights e informações valiosas de grandes volumes de dados. Em um mundo onde os dados estão em toda parte, a Ciência de Dados desempenha um papel crucial no treinamento de modelos de IA e na análise de dados para melhorar a tomada de decisões.
            </p>
            <p>
                Esse campo vai além da simples análise descritiva: ele busca compreender padrões, prever comportamentos futuros e até automatizar decisões por meio de algoritmos inteligentes. O cientista de dados atua como um elo entre os dados e a aplicação prática, sendo responsável por transformar grandes quantidades de informações brutas em <a href="https://www.tinoco.blog.br/post/conhecimentos-acionaveis-dados" target="_blank">conhecimento acionável</a>. Com a crescente digitalização de serviços e o aumento exponencial na geração de dados, a ciência de dados se tornou indispensável para organizações que desejam se manter competitivas, inovadoras e orientadas por dados. A seguir, veremos como essa disciplina se conecta diretamente com a inteligência artificial, tornando possível sua aplicação eficaz em diversos setores.
            </p>
            <h2>Como a Ciência de Dados Contribui para a IA</h2>
            <p>
                A Ciência de Dados fornece as ferramentas e técnicas necessárias para trabalhar com dados estruturados e não estruturados. Desde o processamento de dados até a criação de <strong><a href="https://intellimetri.com.br/algoritmos-preditivos" target="_blank">algoritmos preditivos</a></strong>, a Ciência de Dados é fundamental para treinar sistemas de IA em diversas indústrias, como saúde, educação, finanças e comércio. Além disso, os cientistas de dados também são responsáveis por garantir a <strong>qualidade dos dados</strong> e a integridade dos modelos preditivos.
            </p>
            <p>
                Essa contribuição começa com a <a href="https://g1.globo.com/tecnologia/noticia/2023/11/24/o-que-faz-um-engenheiro-de-dados-profissao-tem-salario-alto-e-deve-bombar-em-2024.ghtml" target="_blank">engenharia de dados</a>, que envolve a coleta, organização e tratamento de grandes volumes de dados provenientes de fontes diversas, como sensores, redes sociais, registros financeiros ou prontuários médicos. Sem dados limpos, organizados e representativos, um modelo de IA dificilmente será confiável. Por isso, os cientistas de dados aplicam técnicas de limpeza, normalização e transformação de dados, além de métodos estatísticos e computacionais para lidar com <a href="https://www.scribbr.co.uk/stats/missing-values/" target="_blank">dados ausentes</a>, <a href="https://estatisticafacil.org/glossario/o-que-e-noise-ruido-estatistica-analise-dados/" target="_blank">ruídos</a> e <a href="https://www.goupdigital.com.br/blog/dados-outliers-o-que-sao" target="_blank">outliers</a>.
            </p>
            <p>
                Outro ponto-chave é a <strong>análise exploratória de dados (<a href="https://books.google.com.br/books?hl=pt-BR&lr=&id=jF8QC-BkhvQC&oi=fnd&pg=PA5&dq=Exploratory+Data+Analysis&ots=KK3vAPEvtH&sig=0WP0G1EyZXOwGXvgt5Xhapw8iAs#v=onepage&q=Exploratory%20Data%20Analysis&f=false" target="_blank">EDA – Exploratory Data Analysis</a>)</strong>, que permite identificar padrões iniciais, correlações e tendências, servindo como base para a construção de <strong>modelos preditivos</strong>. Esses modelos podem ser simples, como <strong><a href="https://www.ibm.com/br-pt/think/topics/linear-regression" target="_blank">regressões lineares</a></strong>, ou mais complexos, como <strong>redes neurais profundas</strong> — e sua performance depende diretamente da compreensão profunda dos <strong><a href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/sms-data-input.html" target="_blank">dados de entrada</a></strong>.
            </p>
            
            <div class="container">
                <img class="imagem" src="imagens/pag-index/ai-hand-700-index.jpg" alt="AI and Human">
            </div>

            <p>
                Na etapa de <a href="https://repositorio.pgsscogna.com.br/bitstream/123456789/65074/1/Modelagem%20de%20Dados.pdf" target="_blank">modelagem</a>, a ciência de dados se conecta diretamente com a IA por meio do uso de <strong>machine learning</strong> e <strong>deep learning</strong>, onde os algoritmos são treinados para realizar previsões, classificações ou segmentações com base em exemplos anteriores. Aqui, os cientistas de dados ajustam hiperparâmetros, avaliam métricas como <strong><a href="https://mariofilho.com/o-que-e-acuracia-em-machine-learning/" target="_blank">acurácia</a></strong>, <strong><a href="https://mariofilho.com/precisao-recall-e-f1-score-em-machine-learning/" target="_blank">precisão</a></strong>, <strong><a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?hl=pt-br" target="_blank">recall</a></strong> e <strong><a href="https://www.datacamp.com/tutorial/auc" target="_blank">AUC</a></strong>, e aplicam técnicas de <strong><a href="https://learn.microsoft.com/pt-br/dotnet/machine-learning/how-to-guides/train-machine-learning-model-cross-validation-ml-net" target="_blank">validação cruzada</a></strong> para garantir que os modelos sejam robustos, generalizáveis e livres de viés.
            </p>
            <p>
                Em setores como <strong><a href="https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-023-04698-z">saúde</a></strong>, a ciência de dados permite treinar modelos que auxiliam no <strong><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8754556/" target="_blank">diagnóstico precoce de doenças</a></strong> ou no <strong><a href="https://futurodasaude.com.br/ia-desenvolvimento-de-medicamentos/" target="_blank">desenvolvimento de medicamentos personalizados</a></strong>. Na <strong><a href="https://www.unesco.org/en/digital-education/artificial-intelligence" target="_blank">educação</a></strong>, viabiliza sistemas de <strong>recomendação <a href="https://onlinedegrees.sandiego.edu/artificial-intelligence-education/" target="_blank">adaptativa</a> para aprendizagem personalizada</strong>. Já no comércio, impulsiona mecanismos de <strong><a href="https://www.ibm.com/think/topics/recommendation-engine#:~:text=A%20recommendation%20engine%2C%20also%20called,items%20based%20on%20those%20patterns." target="_blank">recomendação</a></strong>, <strong><a href="https://www.rapidinnovation.io/post/how-ai-predicts-customer-trends-and-behavior#:~:text=By%20leveraging%20AI%20customer%20behavior,customer%20experiences%20and%20driving%20sales." target="_blank">análise de comportamento do consumidor</a></strong> e <strong><a href="https://mitsloan.mit.edu/ideas-made-to-matter/how-artificial-intelligence-transforming-logistics" target="_blank">otimização logística</a></strong>.
            </p>
            <p>
                Portanto, a ciência de dados não apenas alimenta os sistemas de inteligência artificial com dados de qualidade, mas também garante que esses sistemas sejam <strong>precisos</strong>, <strong>éticos</strong>, <strong>auditáveis</strong> e <strong>eficientes</strong>. É essa integração entre dados, estatística e tecnologia que permite à IA evoluir de maneira <strong>confiável</strong> e <strong>aplicável</strong> no mundo real.
            </p>
            <h1>IA no Mercado de Trabalho: O Futuro das Profissões</h1>
            <h2>Profissões Impactadas pela IA</h2>
            <p>
                A IA já está causando <strong>mudanças profundas no mercado de trabalho</strong>. Profissões que dependem de tarefas repetitivas e baseadas em regras, como <strong><a href="https://www.ibm.com/think/topics/ai-in-marketing" target="_blank">telemarketing</a></strong>, <strong><a href="https://www.ibm.com/think/topics/ai-in-manufacturing" target="_blank">manufatura</a></strong>, <strong><a href="https://www.oracle.com/hk/scm/ai-in-logistics/" target="_blank">logística</a></strong> e até <strong><a href="https://legal.thomsonreuters.com/blog/how-ai-is-transforming-the-legal-profession/" target="_blank">direito</a></strong>, estão sendo fortemente impactadas pela automação. Algoritmos de IA podem substituir funções que requerem análise de grandes volumes de dados e tomada de decisão com base em padrões predefinidos. <br/>
                <p>Exemplo: A IA pode ser utilizada para <strong>automatizar a análise de <a href="https://clp.law.harvard.edu/knowledge-hub/insights/the-impact-of-artificial-intelligence-on-law-law-firms-business-models/" target="_blank">contratos jurídicos</a></strong> ou realizar auditorias financeiras, tarefas que antes eram feitas por humanos.</p>
            </p>
            <p>
                Além disso, áreas como <strong>atendimento ao cliente</strong>, <strong>controle de qualidade</strong> e até <strong>jornalismo</strong> vêm passando por transformações, com sistemas capazes de responder <strong>dúvidas</strong>, <strong>identificar falhas</strong> ou <strong>gerar textos informativos com base em dados</strong>. Embora isso possa gerar receio quanto à substituição de empregos, esse processo não representa apenas perda de postos de trabalho, mas sim uma <strong>reconfiguração do mercado</strong>, onde muitas funções deixam de existir na forma tradicional para dar lugar a novas oportunidades.
            </p>
            <h2> Profissões que Serão Criadas</h2>
            <p>
                Por outro lado, o avanço da IA também gera novas oportunidades de emprego. Profissões como <strong>cientista de dados</strong>, <strong>engenheiro de IA</strong>, <strong>analista de ética em IA</strong>, e <strong>especialista em cibersegurança</strong> estão se tornando cada vez mais demandadas. A <strong>tecnologia está criando um novo paradigma de trabalho</strong>, onde a colaboração entre humanos e máquinas será crucial para o sucesso de muitas indústrias.
            </p>
            <p>
                Além dessas, surgem funções como <strong>treinadores de algoritmos</strong>, <strong>curadores de dados</strong>, <strong>engenheiros de prompt</strong> (especialistas em formular instruções para sistemas de IA generativa), <strong>designers de experiências com IA</strong> e <strong>especialistas em explicabilidade e justiça algorítmica</strong>. Também cresce a demanda por profissionais com conhecimento híbrido, que aliam competências técnicas a habilidades humanas, como pensamento <strong>crítico</strong>, <strong>criatividade</strong> e <strong>empatia</strong> — fundamentais para supervisionar decisões automatizadas e garantir que a tecnologia seja usada de forma <strong>ética</strong> e <strong>responsável</strong>.
            </p>
            <p>
                O futuro do trabalho será cada vez mais orientado à complementaridade entre <strong>capacidades humanas</strong> e <strong>artificiais</strong>, em vez da substituição completa. Assim, adaptar-se às novas exigências, investir em educação contínua e desenvolver competências digitais se tornará essencial para prosperar nesse novo cenário profissional impulsionado pela inteligência artificial.
            </p>
            <p style="text-align: center;"><strong><a href="profissoes.html">Clique aqui</a> e saiba mais sobre profissões que utilizarão IA.</strong> </p>
        </article>
    </main>
    <footer>
        <p>Site criado por Ryan Vasconcellos para o DAC da Universidade Católica Dom Bosco</p>
   </footer>
</body>
</html>